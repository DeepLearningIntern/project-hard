# -*- coding: utf-8 -*-
"""DeepLearningHard-v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xAKGqv17bO-a9QDClUOrJMPPAJ8Ug2ED
"""

# -*- coding: utf-8 -*-
"""Enhanced YOLOv1 implementation with improved accuracy and visualization"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from PIL import Image, ImageDraw, ImageFont, ImageEnhance
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import os
import json
import cv2
from typing import List, Tuple, Dict
import pandas as pd
import zipfile
import shutil
import ast
import random
from collections import defaultdict
import albumentations as A
from albumentations.pytorch import ToTensorV2

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# No classification as per requirements
NUM_CLASSES = 0

class SEBlock(nn.Module):
    """Squeeze-and-Excitation block for channel attention"""
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.excitation = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.squeeze(x)
        y = self.excitation(y)
        return x * y

class ImprovedBasicBlock(nn.Module):
    """Enhanced residual block with SE attention and dropout"""
    def __init__(self, in_channels, out_channels, stride=1, dropout=0.1):
        super(ImprovedBasicBlock, self).__init__()

        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        # SE attention
        self.se = SEBlock(out_channels)

        # Dropout for regularization
        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()

        # Shortcut connection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1,
                         stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        residual = x

        out = F.relu(self.bn1(self.conv1(x)))
        out = self.dropout(out)
        out = self.bn2(self.conv2(out))

        # Apply SE attention
        out = self.se(out)

        out += self.shortcut(residual)
        out = F.relu(out)
        return out

class EnhancedResNet(nn.Module):
    """Enhanced ResNet backbone with better feature extraction"""
    def __init__(self, dropout=0.1):
        super(EnhancedResNet, self).__init__()

        # Initial conv with larger kernel for better feature capture
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        # Enhanced residual layers
        self.layer1 = self._make_layer(64, 64, 3, stride=1, dropout=dropout)
        self.layer2 = self._make_layer(64, 128, 3, stride=2, dropout=dropout)
        self.layer3 = self._make_layer(128, 256, 4, stride=2, dropout=dropout)
        self.layer4 = self._make_layer(256, 512, 3, stride=2, dropout=dropout)

        # Additional conv for feature refinement
        self.feature_refine = nn.Sequential(
            nn.Conv2d(512, 512, kernel_size=3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            SEBlock(512)
        )

    def _make_layer(self, in_channels, out_channels, num_blocks, stride, dropout):
        layers = []
        layers.append(ImprovedBasicBlock(in_channels, out_channels, stride, dropout))
        for _ in range(1, num_blocks):
            layers.append(ImprovedBasicBlock(out_channels, out_channels, 1, dropout))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.feature_refine(x)

        return x

class EnhancedYOLOHead(nn.Module):
    """Enhanced YOLO detection head with better prediction layers"""
    def __init__(self, input_channels=512, grid_size=13, num_boxes=1):
        super(EnhancedYOLOHead, self).__init__()
        self.grid_size = grid_size
        self.num_boxes = num_boxes

        # Feature processing layers
        self.conv_layers = nn.Sequential(
            nn.Conv2d(input_channels, 1024, kernel_size=3, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1, inplace=True),
            nn.Dropout2d(0.2),

            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1, inplace=True),

            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),
            nn.BatchNorm2d(1024),
            nn.LeakyReLU(0.1, inplace=True),
        )

        # Adaptive pooling to get exact grid size
        self.avgpool = nn.AdaptiveAvgPool2d((grid_size, grid_size))

        # Calculate the flattened size
        flattened_size = 1024 * grid_size * grid_size

        # Output size: 5 * num_boxes per grid cell (x, y, w, h, objectness)
        output_size = grid_size * grid_size * num_boxes * 5

        # Enhanced fully connected layers
        self.fc = nn.Sequential(
            nn.Linear(flattened_size, 4096),
            nn.LeakyReLU(0.1, inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 2048),
            nn.LeakyReLU(0.1, inplace=True),
            nn.Dropout(0.3),
            nn.Linear(2048, output_size)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)

        # Reshape to (batch_size, grid_size, grid_size, 5)
        batch_size = x.size(0)
        x = x.view(batch_size, self.grid_size, self.grid_size, 5)

        return x

class EnhancedYOLOv1(nn.Module):
    """Enhanced YOLOv1 model with improved architecture"""
    def __init__(self, grid_size=13, dropout=0.1):
        super(EnhancedYOLOv1, self).__init__()
        self.grid_size = grid_size
        self.backbone = EnhancedResNet(dropout=dropout)
        self.head = EnhancedYOLOHead(input_channels=512, grid_size=grid_size)

    def forward(self, x):
        features = self.backbone(x)
        output = self.head(features)
        return output

class EnhancedPlaneDataset(Dataset):
    """Enhanced dataset with better augmentation using Albumentations"""
    def __init__(self, annotations_file, img_dir, grid_size=13, augment=True, image_size=416):
        self.annotations = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.grid_size = grid_size
        self.augment = augment
        self.image_size = image_size

        # Group annotations by image
        self.image_groups = self.annotations.groupby('filename').apply(
            lambda x: x.to_dict('records'), include_groups=False
        ).to_dict()
        self.image_files = list(self.image_groups.keys())

        # Enhanced augmentation pipeline
        if augment:
            self.transform = A.Compose([
                A.HorizontalFlip(p=0.5),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),
                A.RandomGamma(gamma_limit=(80, 120), p=0.3),
                A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
                A.MotionBlur(blur_limit=3, p=0.2),
                A.Resize(image_size, image_size),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2(),
            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))
        else:
            self.transform = A.Compose([
                A.Resize(image_size, image_size),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2(),
            ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))

        print(f"Dataset statistics: {len(self.annotations)} objects in {len(self.image_files)} images")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.img_dir, img_name)

        # Load image
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        height, width = image.shape[:2]

        # Get annotations
        annotations = self.image_groups[img_name]

        # Convert to absolute coordinates for albumentations
        bboxes = []
        labels = []
        for ann in annotations:
            x_min = ann['x_min'] * width
            y_min = ann['y_min'] * height
            x_max = ann['x_max'] * width
            y_max = ann['y_max'] * height

            # Ensure valid bounding box
            if x_max > x_min and y_max > y_min:
                bboxes.append([x_min, y_min, x_max, y_max])
                labels.append(0)  # Single class (plane)

        # Apply transformations
        if len(bboxes) > 0:
            transformed = self.transform(image=image, bboxes=bboxes, labels=labels)
            image = transformed['image']
            bboxes = transformed['bboxes']
        else:
            transformed = self.transform(image=image, bboxes=[], labels=[])
            image = transformed['image']
            bboxes = []

        # Convert back to normalized coordinates
        normalized_annotations = []
        for bbox in bboxes:
            x_min, y_min, x_max, y_max = bbox
            normalized_annotations.append({
                'x_min': x_min / self.image_size,
                'y_min': y_min / self.image_size,
                'x_max': x_max / self.image_size,
                'y_max': y_max / self.image_size
            })

        # Create target tensor
        target = self.create_enhanced_target(normalized_annotations)

        return image, target

    def create_enhanced_target(self, annotations):
        """Enhanced target creation with better handling of overlapping objects"""
        target = torch.zeros((self.grid_size, self.grid_size, 5))

        # Sort annotations by area (smaller objects first for priority)
        annotations = sorted(annotations, key=lambda x: (x['x_max'] - x['x_min']) * (x['y_max'] - x['y_min']))

        for ann in annotations:
            x_min, y_min, x_max, y_max = ann['x_min'], ann['y_min'], ann['x_max'], ann['y_max']

            # Skip invalid boxes
            if x_max <= x_min or y_max <= y_min:
                continue

            # Calculate center and dimensions
            center_x = (x_min + x_max) / 2
            center_y = (y_min + y_max) / 2
            width = x_max - x_min
            height = y_max - y_min

            # Find responsible grid cell
            grid_x = int(center_x * self.grid_size)
            grid_y = int(center_y * self.grid_size)

            # Ensure within bounds
            grid_x = min(grid_x, self.grid_size - 1)
            grid_y = min(grid_y, self.grid_size - 1)

            # Calculate offsets
            x_offset = (center_x * self.grid_size) - grid_x
            y_offset = (center_y * self.grid_size) - grid_y

            # Assign to grid cell
            target[grid_y, grid_x, 0] = x_offset
            target[grid_y, grid_x, 1] = y_offset
            target[grid_y, grid_x, 2] = width
            target[grid_y, grid_x, 3] = height
            target[grid_y, grid_x, 4] = 1.0

        return target

def enhanced_yolo_loss(predictions, targets, lambda_coord=5.0, lambda_noobj=0.5, lambda_size=1.0):
    """Enhanced YOLO loss with better size handling"""
    batch_size = predictions.shape[0]

    # Apply activations
    pred_xy = torch.sigmoid(predictions[..., :2])
    pred_wh = predictions[..., 2:4]  # Keep raw for better gradient flow
    pred_conf = torch.sigmoid(predictions[..., 4])

    # Extract targets
    target_xy = targets[..., :2]
    target_wh = targets[..., 2:4]
    target_conf = targets[..., 4]

    # Object and no-object masks
    obj_mask = target_conf == 1
    noobj_mask = target_conf == 0

    # Coordinate loss (smoother L1 loss)
    xy_loss = F.smooth_l1_loss(
        pred_xy[obj_mask.unsqueeze(-1).expand_as(pred_xy)],
        target_xy[obj_mask.unsqueeze(-1).expand_as(target_xy)],
        reduction='sum'
    )

    # Size loss with logarithmic scale for better small object detection
    wh_loss = F.smooth_l1_loss(
        torch.log(torch.clamp(torch.sigmoid(pred_wh) + 1e-6, min=1e-6))[obj_mask.unsqueeze(-1).expand_as(pred_wh)],
        torch.log(torch.clamp(target_wh + 1e-6, min=1e-6))[obj_mask.unsqueeze(-1).expand_as(target_wh)],
        reduction='sum'
    )

    # Confidence loss with focal loss for hard examples
    alpha = 0.25
    gamma = 2.0

    # Object confidence loss
    obj_conf_loss = F.binary_cross_entropy(pred_conf[obj_mask], target_conf[obj_mask], reduction='none')
    pt_obj = pred_conf[obj_mask]
    focal_weight_obj = alpha * (1 - pt_obj) ** gamma
    obj_conf_loss = (focal_weight_obj * obj_conf_loss).sum()

    # No-object confidence loss
    noobj_conf_loss = F.binary_cross_entropy(pred_conf[noobj_mask], target_conf[noobj_mask], reduction='none')
    pt_noobj = 1 - pred_conf[noobj_mask]
    focal_weight_noobj = (1 - alpha) * (1 - pt_noobj) ** gamma
    noobj_conf_loss = (focal_weight_noobj * noobj_conf_loss).sum()

    # Total loss
    total_loss = (
        lambda_coord * xy_loss +
        lambda_size * wh_loss +
        obj_conf_loss +
        lambda_noobj * noobj_conf_loss
    ) / batch_size

    return total_loss, {
        'xy_loss': xy_loss.item() / batch_size,
        'wh_loss': wh_loss.item() / batch_size,
        'obj_loss': obj_conf_loss.item() / batch_size,
        'noobj_loss': noobj_conf_loss.item() / batch_size
    }

def enhanced_transform_output(predictions, grid_size=13, img_width=416, img_height=416,
                            conf_threshold=0.3, nms_threshold=0.4):
    """Enhanced output transformation with better confidence handling"""
    boxes = []
    scores = []

    # Apply activations
    pred_xy = torch.sigmoid(predictions[..., :2])
    pred_wh = torch.sigmoid(predictions[..., 2:4])
    pred_conf = torch.sigmoid(predictions[..., 4])

    for i in range(grid_size):
        for j in range(grid_size):
            objectness = pred_conf[i, j].item()

            if objectness > conf_threshold:
                # Get coordinates
                x_offset = pred_xy[i, j, 0].item()
                y_offset = pred_xy[i, j, 1].item()
                width = pred_wh[i, j, 0].item()
                height = pred_wh[i, j, 1].item()

                # Convert to absolute coordinates
                center_x = (j + x_offset) / grid_size
                center_y = (i + y_offset) / grid_size

                # Convert to pixel coordinates
                center_x_px = center_x * img_width
                center_y_px = center_y * img_height
                width_px = width * img_width
                height_px = height * img_height

                # Convert to corner coordinates
                x_min = max(0, center_x_px - width_px / 2)
                y_min = max(0, center_y_px - height_px / 2)
                x_max = min(img_width, center_x_px + width_px / 2)
                y_max = min(img_height, center_y_px + height_px / 2)

                # Skip invalid boxes
                if x_max > x_min and y_max > y_min:
                    boxes.append([x_min, y_min, x_max, y_max])
                    scores.append(objectness)

    return boxes, scores

def enhanced_nms(boxes, scores, iou_threshold=0.4):
    """Enhanced NMS with better IoU calculation"""
    if len(boxes) == 0:
        return [], []

    boxes = np.array(boxes, dtype=np.float32)
    scores = np.array(scores, dtype=np.float32)

    # Calculate areas
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])

    # Sort by scores
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)

        if order.size == 1:
            break

        # Calculate IoU
        xx1 = np.maximum(boxes[i, 0], boxes[order[1:], 0])
        yy1 = np.maximum(boxes[i, 1], boxes[order[1:], 1])
        xx2 = np.minimum(boxes[i, 2], boxes[order[1:], 2])
        yy2 = np.minimum(boxes[i, 3], boxes[order[1:], 3])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        intersection = w * h

        union = areas[i] + areas[order[1:]] - intersection
        iou = intersection / (union + 1e-6)

        # Keep boxes with IoU less than threshold
        inds = np.where(iou <= iou_threshold)[0]
        order = order[inds + 1]

    return boxes[keep].tolist(), scores[keep].tolist()

def enhanced_plot_predictions(image, boxes, scores, title="Enhanced YOLO Predictions",
                            conf_threshold=0.3, save_path=None):
    """Enhanced visualization with better styling and information display"""
    fig, ax = plt.subplots(1, 1, figsize=(15, 15))

    # Convert tensor to numpy if needed
    if torch.is_tensor(image):
        # Denormalize
        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
        image = image * std + mean
        image = torch.clamp(image, 0, 1)
        image = image.permute(1, 2, 0).cpu().numpy()

    ax.imshow(image)

    # Color palette for different confidence levels
    colors = ['red', 'orange', 'yellow', 'green', 'blue']

    high_conf_boxes = []
    medium_conf_boxes = []
    low_conf_boxes = []

    # Categorize boxes by confidence
    for box, score in zip(boxes, scores):
        if score >= 0.7:
            high_conf_boxes.append((box, score, 'red', 3))
        elif score >= 0.5:
            medium_conf_boxes.append((box, score, 'orange', 2))
        else:
            low_conf_boxes.append((box, score, 'yellow', 1))

    # Draw boxes with different styles based on confidence
    all_boxes = high_conf_boxes + medium_conf_boxes + low_conf_boxes

    for box, score, color, linewidth in all_boxes:
        x_min, y_min, x_max, y_max = box
        width = x_max - x_min
        height = y_max - y_min

        # Create rectangle
        rect = patches.Rectangle((x_min, y_min), width, height,
                               linewidth=linewidth, edgecolor=color, facecolor='none')
        ax.add_patch(rect)

        # Add label with confidence
        label = f'Plane: {score:.3f}'

        # Choose text background color based on confidence
        bg_color = color
        text_color = 'white' if color in ['red', 'blue'] else 'black'

        ax.text(x_min, y_min - 5, label,
                bbox=dict(facecolor=bg_color, alpha=0.8, edgecolor='black'),
                fontsize=12, color=text_color, weight='bold')

    # Add legend
    legend_elements = [
        patches.Patch(color='red', label=f'High Confidence (â‰¥0.7): {len(high_conf_boxes)}'),
        patches.Patch(color='orange', label=f'Medium Confidence (â‰¥0.5): {len(medium_conf_boxes)}'),
        patches.Patch(color='yellow', label=f'Low Confidence (<0.5): {len(low_conf_boxes)}')
    ]
    ax.legend(handles=legend_elements, loc='upper right', fontsize=10)

    # Enhanced title with statistics
    total_detections = len(boxes)
    avg_confidence = np.mean(scores) if scores else 0

    ax.set_title(f"{title}\nTotal Detections: {total_detections} | "
                f"Average Confidence: {avg_confidence:.3f} | "
                f"Threshold: {conf_threshold}",
                fontsize=14, weight='bold', pad=20)

    ax.axis('off')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"Visualization saved to {save_path}")

    plt.show()

    return fig

def train_enhanced_model(model, train_loader, val_loader=None, num_epochs=100,
                        learning_rate=0.001, save_path='enhanced_yolo.pth'):
    """Enhanced training with better optimization and monitoring"""
    model = model.to(device)

    # Initialize weights
    def init_weights(m):
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, 0, 0.01)
            nn.init.constant_(m.bias, 0)

    model.apply(init_weights)

    # Enhanced optimizer with weight decay
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate,
                           weight_decay=0.0001, betas=(0.9, 0.999))

    # Cosine annealing scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2, eta_min=1e-6
    )

    best_val_loss = float('inf')
    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        epoch_loss = 0.0
        epoch_losses = {'xy': 0, 'wh': 0, 'obj': 0, 'noobj': 0}

        for batch_idx, (images, targets) in enumerate(train_loader):
            images = images.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()

            # Forward pass
            predictions = model(images)

            # Calculate loss
            loss, loss_components = enhanced_yolo_loss(predictions, targets)

            # Backward pass
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)

            optimizer.step()

            epoch_loss += loss.item()
            for key in epoch_losses:
                epoch_losses[key] += loss_components[f'{key}_loss']

            if batch_idx % 20 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item():.4f}')

        avg_loss = epoch_loss / len(train_loader)
        train_losses.append(avg_loss)

        # Print epoch statistics
        print(f'\nEpoch {epoch+1}/{num_epochs} - Training Loss: {avg_loss:.4f}')
        print(f'Loss Components - XY: {epoch_losses["xy"]/len(train_loader):.4f}, '
              f'WH: {epoch_losses["wh"]/len(train_loader):.4f}, '
              f'Obj: {epoch_losses["obj"]/len(train_loader):.4f}, '
              f'NoObj: {epoch_losses["noobj"]/len(train_loader):.4f}')

        # Validation phase
        if val_loader:
            model.eval()
            val_loss = 0.0
            val_detections = 0
            val_ground_truth = 0

            with torch.no_grad():
                for images, targets in val_loader:
                    images = images.to(device)
                    targets = targets.to(device)

                    predictions = model(images)
                    loss, _ = enhanced_yolo_loss(predictions, targets)
                    val_loss += loss.item()

                    # Count detections
                    pred_conf = torch.sigmoid(predictions[..., 4])
                    val_detections += (pred_conf > 0.3).sum().item()
                    val_ground_truth += (targets[..., 4] == 1).sum().item()

            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)

            detection_rate = val_detections / max(val_ground_truth, 1)
            print(f'Validation Loss: {avg_val_loss:.4f}, Detection Rate: {detection_rate:.3f}')

            # Save best model
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'epoch': epoch,
                    'val_loss': avg_val_loss,
                    'grid_size': model.grid_size,
                }, save_path)
                print(f'âœ… Saved best model with validation loss: {avg_val_loss:.4f}')

        # Update learning rate
        scheduler.step()

        # Visualize predictions every 15 epochs
        if (epoch + 1) % 15 == 0 and val_loader:
            print("\nðŸ” Visualizing current predictions...")
            visualize_predictions(model, val_loader, num_samples=2)

        print("-" * 50)

    return model, train_losses, val_losses

def visualize_predictions(model, data_loader, num_samples=3, conf_threshold=0.25):
    """Visualize model predictions on sample images"""
    model.eval()

    with torch.no_grad():
        for i, (images, targets) in enumerate(data_loader):
            if i >= num_samples:
                break

            # Get prediction for first image in batch
            image = images[0]
            target = targets[0]

            predictions = model(images[:1].to(device))
            pred = predictions[0].cpu()

            # Transform predictions to bounding boxes
            boxes, scores = enhanced_transform_output(
                pred, grid_size=model.grid_size,
                img_width=416, img_height=416,
                conf_threshold=conf_threshold
            )

            # Apply NMS
            if len(boxes) > 0:
                boxes, scores = enhanced_nms(boxes, scores, iou_threshold=0.4)

            # Plot results
            enhanced_plot_predictions(
                image, boxes, scores,
                title=f"Sample {i+1} - Predictions vs Ground Truth",
                conf_threshold=conf_threshold
            )

            # Also show ground truth
            gt_boxes = []
            for y in range(model.grid_size):
                for x in range(model.grid_size):
                    if target[y, x, 4] == 1:  # Object present
                        # Convert back to pixel coordinates
                        center_x = (x + target[y, x, 0]) / model.grid_size * 416
                        center_y = (y + target[y, x, 1]) / model.grid_size * 416
                        width = target[y, x, 2] * 416
                        height = target[y, x, 3] * 416

                        x_min = center_x - width / 2
                        y_min = center_y - height / 2
                        x_max = center_x + width / 2
                        y_max = center_y + height / 2

                        gt_boxes.append([x_min, y_min, x_max, y_max])

            if gt_boxes:
                enhanced_plot_predictions(
                    image, gt_boxes, [1.0] * len(gt_boxes),
                    title=f"Sample {i+1} - Ground Truth",
                    conf_threshold=0.0
                )

def evaluate_enhanced_model(model, test_image_path, conf_threshold=0.25,
                          nms_threshold=0.4, save_result=True):
    """Enhanced model evaluation with detailed analysis"""
    model.eval()

    # Load and preprocess image
    image = cv2.imread(test_image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    original_image = image.copy()

    # Preprocess
    transform = A.Compose([
        A.Resize(416, 416),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])

    transformed = transform(image=image)
    image_tensor = transformed['image'].unsqueeze(0).to(device)

    with torch.no_grad():
        predictions = model(image_tensor)
        predictions = predictions.cpu().squeeze(0)

    # Transform predictions
    boxes, scores = enhanced_transform_output(
        predictions, grid_size=model.grid_size,
        img_width=416, img_height=416,
        conf_threshold=conf_threshold
    )

    print(f"Initial detections: {len(boxes)}")

    # Apply NMS
    if len(boxes) > 0:
        filtered_boxes, filtered_scores = enhanced_nms(
            boxes, scores, iou_threshold=nms_threshold
        )
        print(f"After NMS: {len(filtered_boxes)} detections")
    else:
        filtered_boxes, filtered_scores = [], []

    # Debug information
    pred_conf = torch.sigmoid(predictions[..., 4])
    max_conf = pred_conf.max().item()
    print(f"Max confidence: {max_conf:.4f}")
    print(f"Cells with conf > 0.1: {(pred_conf > 0.1).sum().item()}")

    # Create visualization
    fig = enhanced_plot_predictions(
        transform(image=original_image)['image'],
        filtered_boxes, filtered_scores,
        title=f"Enhanced YOLO Detection Results",
        conf_threshold=conf_threshold,
        save_path=f"detection_result_{os.path.basename(test_image_path)}" if save_result else None
    )

    return filtered_boxes, filtered_scores, fig

# Utility functions for dataset preparation
def extract_dataset_zip(zip_path='dataset.zip', extract_to='./'):
    """Extract dataset.zip file"""
    print(f"Extracting dataset from {zip_path}...")

    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)

        # Find annotations and images
        annotations_file = None
        images_dir = None

        for root, dirs, files in os.walk(extract_to):
            for file in files:
                if file.lower() == 'annotations.csv':
                    annotations_file = os.path.join(root, file)
                    break

        for root, dirs, files in os.walk(extract_to):
            for dir_name in dirs:
                if dir_name.lower() in ['images', 'image', 'img', 'imgs']:
                    images_dir = os.path.join(root, dir_name)
                    break

        print(f"Found annotations: {annotations_file}")
        print(f"Found images: {images_dir}")

        return annotations_file, images_dir

    except Exception as e:
        print(f"Error extracting dataset: {e}")
        return None, None

def convert_polygon_to_bbox(geometry_str):
    """Convert polygon to bounding box"""
    try:
        coordinates = ast.literal_eval(geometry_str)
        x_coords = [coord[0] for coord in coordinates]
        y_coords = [coord[1] for coord in coordinates]

        x_min = min(x_coords)
        x_max = max(x_coords)
        y_min = min(y_coords)
        y_max = max(y_coords)

        return x_min, y_min, x_max, y_max

    except Exception as e:
        print(f"Error parsing geometry: {e}")
        return None

def convert_annotations_to_yolo_format(annotations_file, images_dir, output_file):
    """Convert polygon annotations to YOLO format"""
    print("Converting annotations to YOLO format...")

    df = pd.read_csv(annotations_file)
    print(f"Original annotations: {len(df)} entries")

    def get_image_dimensions(image_path):
        try:
            with Image.open(image_path) as img:
                return img.size
        except:
            return None

    converted_data = []

    unique_images = df['image_id'].unique()
    print(f"Processing {len(unique_images)} unique images...")

    for image_id in unique_images:
        image_path = os.path.join(images_dir, image_id)

        if not os.path.exists(image_path):
            continue

        dimensions = get_image_dimensions(image_path)
        if dimensions is None:
            continue

        img_width, img_height = dimensions

        image_annotations = df[df['image_id'] == image_id]

        for _, row in image_annotations.iterrows():
            bbox = convert_polygon_to_bbox(row['geometry'])
            if bbox is None:
                continue

            x_min, y_min, x_max, y_max = bbox

            # Normalize coordinates
            x_min_norm = x_min / img_width
            y_min_norm = y_min / img_height
            x_max_norm = x_max / img_width
            y_max_norm = y_max / img_height

            # Ensure coordinates are within [0, 1]
            x_min_norm = max(0, min(1, x_min_norm))
            y_min_norm = max(0, min(1, y_min_norm))
            x_max_norm = max(0, min(1, x_max_norm))
            y_max_norm = max(0, min(1, y_max_norm))

            converted_data.append({
                'filename': image_id,
                'x_min': x_min_norm,
                'y_min': y_min_norm,
                'x_max': x_max_norm,
                'y_max': y_max_norm
            })

    converted_df = pd.DataFrame(converted_data)
    converted_df.to_csv(output_file, index=False)

    print(f"Converted {len(converted_data)} annotations")
    return output_file

def main():
    """Enhanced main training pipeline with Google Drive integration"""
    print("ðŸš€ Starting Enhanced YOLOv1 Training Pipeline")

    # Mount Google Drive
    from google.colab import drive
    drive.mount('/content/drive')
    print("Google Drive mounted.")

    # Path to dataset
    zip_file_path_in_drive = '/content/drive/My Drive/YOLO_Project/hard/Last/dataset.zip'

    # Extract dataset
    extract_destination = './extracted_dataset'
    annotations_file, images_dir = extract_dataset_zip(
        zip_path=zip_file_path_in_drive,
        extract_to=extract_destination
    )

    if not annotations_file or not images_dir:
        print("Failed to extract dataset!")
        return

    # Convert annotations if needed
    converted_file = 'converted_annotations.csv'
    df_check = pd.read_csv(annotations_file)
    if 'geometry' in df_check.columns:
        annotations_file = convert_annotations_to_yolo_format(
            annotations_file, images_dir, converted_file
        )

    # Hyperparameters
    GRID_SIZE = 13
    BATCH_SIZE = 8 if torch.cuda.is_available() else 4
    NUM_EPOCHS = 50
    LEARNING_RATE = 0.001
    IMAGE_SIZE = 416

    # Create enhanced dataset
    train_dataset = EnhancedPlaneDataset(
        annotations_file=annotations_file,
        img_dir=images_dir,
        grid_size=GRID_SIZE,
        augment=True,
        image_size=IMAGE_SIZE
    )

    print(f"ðŸ“Š Dataset loaded: {len(train_dataset)} samples")

    # Split dataset for validation
    train_size = int(0.8 * len(train_dataset))
    val_size = len(train_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        train_dataset, [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    # Create data loaders with memory-efficient settings
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                            num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                          num_workers=2, pin_memory=True)

    # Create enhanced model
    model = EnhancedYOLOv1(grid_size=GRID_SIZE, dropout=0.1)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"ðŸ”§ Enhanced model created:")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,}")
    print(f"   Grid size: {GRID_SIZE}x{GRID_SIZE}")

    # Train model with optimized settings
    print("ðŸŽ¯ Starting enhanced training...")
    trained_model, train_losses, val_losses = train_enhanced_model(
        model, train_loader, val_loader,
        num_epochs=NUM_EPOCHS,
        learning_rate=LEARNING_RATE,
        save_path='enhanced_yolo_best.pth'
    )

    # Save final model
    torch.save({
        'model_state_dict': trained_model.state_dict(),
        'grid_size': GRID_SIZE,
        'total_params': total_params
    }, 'yolo_model_enhanced_final.pth')
    print("ðŸ’¾ Enhanced model saved as 'yolo_model_enhanced_final.pth'")

    print("âœ… Training completed!")

    # Plot training curves
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)
    if val_losses:
        plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 3, 2)
    if len(train_losses) > 20:
        plt.plot(train_losses[-20:], label='Training Loss (Last 20)', color='blue', linewidth=2)
        if val_losses:
            plt.plot(val_losses[-20:], label='Validation Loss (Last 20)', color='red', linewidth=2)
        plt.xlabel('Recent Epochs')
        plt.ylabel('Loss')
        plt.title('Recent Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.subplot(1, 3, 3)
    if len(train_losses) > 10:
        smoothed_train = np.convolve(train_losses, np.ones(5)/5, mode='valid')
        plt.plot(smoothed_train, label='Smoothed Training', color='blue', linewidth=2)
        if val_losses and len(val_losses) > 10:
            smoothed_val = np.convolve(val_losses, np.ones(5)/5, mode='valid')
            plt.plot(smoothed_val, label='Smoothed Validation', color='red', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Smoothed Loss')
        plt.title('Smoothed Progress')
        plt.legend()
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('enhanced_training_progress.png', dpi=150, bbox_inches='tight')
    plt.show()

    # Test on sample images with enhanced visualization
    print("\nðŸ” Testing enhanced model on sample images...")
    sample_images = []
    for root, dirs, files in os.walk(images_dir):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                sample_images.append(os.path.join(root, file))
                if len(sample_images) >= 5:
                    break
        if len(sample_images) >= 5:
            break

    if sample_images:
        results = []
        for i, img_path in enumerate(sample_images[:5]):
            print(f"\nTesting on: {os.path.basename(img_path)}")
            boxes, scores, fig = evaluate_enhanced_model(
                trained_model, img_path,
                conf_threshold=0.25,
                nms_threshold=0.4,
                save_result=True
            )
            results.append((img_path, len(boxes), np.mean(scores) if scores else 0))
            print(f"âœ… Detected {len(boxes)} planes with avg confidence: {np.mean(scores):.3f}" if scores else f"âœ… Detected {len(boxes)} planes")

        # Print enhanced summary
        print("\n" + "="*60)
        print("ðŸŽ¯ ENHANCED TEST RESULTS SUMMARY")
        print("="*60)
        total_detections = sum(result[1] for result in results)
        avg_detections = total_detections / len(results)

        for i, (img_path, count, avg_conf) in enumerate(results):
            confidence_text = f" (avg conf: {avg_conf:.3f})" if avg_conf > 0 else ""
            print(f"{i+1}. {os.path.basename(img_path)}: {count} planes{confidence_text}")

        print(f"\nTotal detections: {total_detections}")
        print(f"Average detections per image: {avg_detections:.1f}")
        print("="*60)

if __name__ == "__main__":
    main()